# 쿠버네티스(k8s)

컨테이너 기반의 애플리케이션을 개발하고 배포할 수 있도록 설계된 오픈 소스 플랫폼  
컨테이너 오케스트레이션 도구의 사실상 표준  
분산 시스템을 탄력적으로 실행하기 위한 프레임 워크를 제공  

- **서비스 디스커버리와 로드 밸런싱**   
- **스토리지 오케스트레이션**   
- **자동화된 롤아웃과 롤백**   
- **자동화된 빈 패킹(bin packing)**   
- **자동화된 복구(self-healing)**   
- **시크릿과 구성 관리**   

## 쿠버네티스 컴포넌트
쿠버네티스 클러스터는 컨테이너화 된 애플리캐이션을 실행하는 노드의 집합.  

![Alt text](/statics/image.png)
![Alt text](/statics/image2.png)

### 마스터 노드(컨트롤 플레인 컴포넌트)
클러스터에 관한 전반적인 결정. 제어판의 역할-control plane   
- kube-apiserver
- etcd
- kube-scheduler
- kube-controller-manager
- cloud-controller-manager

### 워커 노드(노드 컴포넌트)
- kubelet
- kube-proxy
- 컨테이너 런타임

### Cluster API
쿠버네티스 컨트롤 플레인의 프론트엔드  
사용자가 쿠버네티스 클러스터와 상호 작용하는 방식  
쿠버네티스 클러스터 구성을 선언적인 형태로 작성하고 자동으로 생성하고 관리하는 방법을 제공  
  -> 라이프사이클 관리

다양한 인프라 환경에서 일관되고 반복적인 클러스터 배포 가능  
외부에서 어떤 기능을 호출했을 때 그에 따른 로직을 모아둔 곳에서 특정 함수에 대한 기능을 보내준다

### 배포 과정
![Alt text](/statics/image1.png)

* app descriptor: 내가 어떻게 묶을 것인지, 어떻게 구성할지 기재한 명세서
* kudectl을 통해 control plane으로 전달 -> node에 분배
* 어디에 배치할지는 Scheduler가 결정하지만 실제로 배치를 명령하는 것은 Kubelet의 역할
* 이 상태들은 control plane 안에 있는 etcd에 저장

### kubectl
쿠버네티스 클러스터를 관리하는 명령행 도구(CLI)  
로컬 환경, 매니지드 환경 모두 사용 가능  
쿠버네티스 자원을 생성, 업데이트, 삭제(create, update, delete)  
디버그, 모니터링, 트러블 슈팅(log, exec, cp, top, attach…)  
클러스터 관리(cordon, top, drain, taint…)  


# 쿠버네티스 리소스
**node**	컨테이너를 배치하는 서버   
**namespace**	쿠버네티스 클러스터 안의 가상 클러스터  
단일 클러스터 내에서의 리소스 그룹 격리 메커니즘  
**pod**	컨테이너 집합 중 가장 작은 단위로 컨테이너의 실행 방법을 정의. 컨테이너와 볼륨을 묶은 것  
**replicaset**   같은 스펙을 갖는 파드를 여러 개 생성하고 관리  
**deployment**  replicaset의 리비전(버전)을 관리  
**service** 파드에 집합에 접근하기 위한 경로를 정의   
**ingress** 서비스를 쿠버네티스 클러스트의 외부로 노출(통합)  
**configmap** 설정정보 저장   
**persistentvolume** 파드가 사용할 스토리지의 크기 및 종류를 정의  
**persistentvolumeclaim**  퍼시스턴트 볼륨을 동적으로 확보  
**job**  상주 실행을 목적으로 하지 않는 파드를 여러 개 생성하고 정상적인 종료를 보장  
**clonejob** cron문법으로 스케쥴링되는 job. 정해진 시간에 동작  
**secrets** 인증정보와 같은 기밀 데이터를 정의하고 전해줌  

## 파드(pod)
- 컨테이너 애플리케이션의 기본 단위  
- **단일 목적**을 수행하기 위한 **한 개 이상의 컨테이너로 구성**된 컨테이너의 집합
- 여러 리눅스 네임스페이스를 공유하는 **여러 컨테이너들을 추상화된 집합으로 사용**하기 위함

> **초기화 컨테이너(init container)**   
> 파드의 앱 컨테이너들이 실행되기 전에 실행되는 특수한 컨테이너  
> 앱 이미지에는 없는 셋업을 위한 유틸리티 또는 설정 스크립트 등을 포함할 수 있음  
> 이번의 경우에는 사이드카 패턴에서 쓰인 공유볼륨에 날짜별로 디렉토리생성 후 소유자권한을 바꿔놓는? 미리 세팅하는 개념  
> 
> **일반적인 컨테이너와의 차이점**
> - 앱 컨테이너의 리소스 상한, 볼륨, 보안 셋팅을 포함한 모든 필드와 기능을 지원
> - 파드가 준비상태가 되기 전에 완료를 목표로 실행되어야 하므로, lifecycle, livenessProbe, readinessProbe, startupProbe을 지원하지 않음
> 
### 파드의 헬스 체크 기능

파드와 컨테이너에는 애플리케이션이 정상적으로 기동하고 있는지 확인하는 기능(=헬스 체크 기능)을 설정할 수 있으며 이상이 감지되면 컨테이너를 강제 종료하고 재시작할 수 있음(kubelet이 담당)

kubelet의 헬스 체크는 활성 프로브(Liveness Probe)와 준비 상태 프로브(Readiness Probe)를 사용하여 실행 중인 파드의 컨테이너를 검사

**활성 프로브(Liveness Probe)**

- 컨테이너의 애플리케이션이 **정상적으로 실행** 중인 것을 검사
- 검사에 실패하면 파드 상의 컨테이너를 강제로 종료하고 재시작
- 이 기능을 사용하기 위해서는 매니페스트에 명시적으로 설정해야 함

**준비 상태 프로브(Readiness Probe)**

- 컨테이너의 애플리케이션이 **요청을 받을 준비**가 되었는지를 검사

## 레플리카셋(replicaset)
**일정 개수의 파드를 유지**하는 컨트롤러
- 정해진 수의 동일한 파드가 항상 실행되도록 관리
- 노드 장애 등의 이유로 파드를 사용할 수 없다면 다른 노드에 파드를 다시 생성

시크릿 생성 및 확인(pull 횟수로 인해 안되는 경우)   
도커 허브에서 이미지를 가져올 때 사용할 자격증명 정보를 저장할 시크릿을 생성  
컨테이너가 죽으면 파드가 살려주는데 파드가 죽으면 모니터링 관리자가 확인하기 전까지 모름! 그래서 나온게 레플리카셋!

### 레플리카셋의 동작 원리

- 라벨 셀렉터(Label Selector)를 이용해서 유지할 파드를 정의
- 레플리카셋은 spec.selector.macthLabels에 정의된 라벨을 통해 생성해야 하는 파드를 찾음
→ app: my-nginx-pods-label
- 라벨을 가지는 파드의 개수가 replicas 항목에 정의된 숫자보다 적으면 파드를 정의하는 파드 템플릿(template) 내용으로 파드를 생성
  
> **노드가 죽는다면?**   
> 레플리카셋이 노드가 죽은것을 인지하고 마스터에 있던 etcd에  남아있는 파드에 대한 기록을 확인한다. 그 기록에서 해당 노드에 있던 파드들(노드가 죽어 연결되지 않는 파드들)을 다른 노드에 생성한다!

## 디플로이먼트(Deployment)

- 레플리카셋 관리+파드 배포에 최적화(버전관리)
- 애플리케이션의 **배포와 업데이트**를 편하게 하기 위해서 사용!
- 쿠버네티스에서 **상태가 없는(stateless) 애플리케이션을 배포**할 때 사용하는 가장 기본적인 컨트롤러


> **stateless**   
> 상태: 요청과 응답간의 관계  
> 요청1(선행요청)이 가고 요청2가 갔을 때 이 두개의 요청간의 관계를 안다면 상태가 있다고 전제  
> 선후관계를 따지지않고 요청에 맞는 처리만 해주는 것 모든 요청을 동등하게 보는 것

- 디플로이먼트는 **스케일, 롤아웃, 롤백, 자동복구** 기능이 있음

**스케일** 파드의 개수를 늘리거나 줄일 수 있음  
**롤아웃, 롤백** 서비스를 유지하면서 파드를 교체  
**자동복구** 노드 수준에서 장애가 발생했을 때 파드를 복구하는 것이 가능

### 디플로이먼트를 사용하는 이유 1. 스케일

- 레플리카의 값을 변경해서 파드의 개수를 조절
  → 처리능력을 높이고 낮추는 기능
- 파드의 개수를 늘리는 중에 쿠버네티스 클러스터의 자원(CPU,  메모리 등)이 부족해지면 노드를 추가하여 자원이 생길때 까지 파드 생성을 **보류**


### 디플로이먼트를 사용하는 이유 2. 롤아웃, 롤백

애플리케이션을 업데이트할 때 레플리카셋의 변경사항을 저장하는 **리비전(revision)을 남겨 롤백을 가능**하게 해주고, 무중단서비스를 위해 **파드의 롤링 업데이트 전략(배포전략)**을 지정할 수 있음

### 디플로이먼트를 사용하는 이유 3. 자동복구

파드 내의 컨테이너가 종료되는 경우, 파드는 컨테이너 수준의 장애에 대해 자동복구를 시도하고, 디플로이먼트는 파드 단위로 복구를 시도

### 디플로이먼트 배포 전략

**인플레이스 배포 (In-place Deployment)**

현재 제공되고 있는 서비스 전체를 중지시킨 후 새로운 버전을 **동시**에 올림
![Alt text](/statics/recreate.png)

**롤링 배포 (Rolling Update Deployment)**

구 버전에서 새 버전으로 트래픽을 점진적으로 전환하며, 구 버전의 인스턴스도 점차 삭제됩니다. 그렇기 때문에 **서버 수의 제약이 있을 경우에는 유용한 방법**이 될 수 있지만 배포 중 인스턴스의 수가 감소 되기 때문에 서버 처리 용량을 미리 고려해야함

![Alt text](/statics/RollingUpdate.png)

**블루/그린 배포 (Blue/Green Deployment)**

블루→ 지금 서비스중인 것 / 그린→개발후 서비스 전환할 것

똑같은 시스템이 두개가 있을 때, 블루를 그린으로 전환. 서비스의 중단없이 바로 전환가능하지만 동일한 시스템이 두 개 있어야하기 때문에 리소스를 많이 잡아먹는다는 단점이 있다

**카나리 배포 (Canary Deployment)**

전체 시스템이 있을 때 그 중 일부만 새로운 시스템으로 유입시킴(시범케이스)

오류가 나지 않는다면 나머지 요청도 새로운 시스템으로 점차 유입시켜 전환한다



# 오브젝트(objects)


## 컨트롤러(controler)
파드의 실행을 제어하는 오브젝트
![Alt text](/statics/imagecontroler.png)

### 데몬셋(DaemonSet)

각 노드에 파드를 하나씩 배치하는 컨트롤러(리소스)   
→ 레플리카 수를 지정할수 없고 하나의 노드에 두 개의 파드를 배치할 수 없다   
→ 단, 파드를 배치하지 않을 노드 정의는 가능함(nodeSelector, 노드 안티어피니티 사용)   
→ 노드를 늘렸을 때 데몬셋의 파드도 자동으로 늘어난 노드에서 기능   
→ **호스트 단위로 로그를 수집하는 경우, 리소스 사용현황 및 노드 상태를 모니터링시 사용**  

### **데몬셋 업데이트 전략**   
**OnDelete**  
- 데몬셋 매니페스트가 변경되어도 기존 파드를 업데이트하지 않고, 파드가 다시 생성될 때 새롭게 정의한 파드를 생성

**RollingUpdate**
- 기본값으로 즉시 파드를 업데이트할 때 사용
- maxSurge(동시에 생성할 수 있는 최대 파드 수)를 설정할 수 없으며, maxUnavailable(동시에 정지 가능한 최대 파드 수)만 설정할 수 있음
- maxUnavailable의 기본값은 1이며, 0으로 지정할 수 없음  

# 서비스(Service)
파드를 연결하고 외부에 노출시킴  
→  클러스터 외부에서 클러스터 내부의 파드에 접근할 수 있게 해주는 것!

### **서비스 기능**
- 여러 개의 파드에 쉽게 접근할 수 있도록 고유한 **도메인 이름을 부여**
- 여러 개의 파드에 접근할 때, 요청을 분산하는 **로드 밸런서** 기능을 수행
- 클라우드 플랫폼의 로드 밸런서, 클러서 노드의 포트 등을 통해 **파드를 외부에 노출**

### 서비스 타입

**ClusterIP(Default)**

- 클러스터 내부에서 파드들에 접근할 때 사용
- 외부로 파드를 노출하지 않기 때문에 클러스터 내부에서만 사용되는 파드에 적합

**NodePort**

- 파드에 접근할 수 있는 포트를 클러스터의 모든 노드에 동일하게 개방
- **외부에서 파드에 접근할 수 있는 서비스 타입**
- 접근할 수 있는 포트는 랜덤으로 정해지지만 특정ㅇ 포트로 접근하도록 설정할 수 있음

**LoadBalancer**

- 클라우드 플랫폼에서 제공하는 로드밸런서를 동적으로 프로비저닝해 파드에 연결
- NodePort 타입과 마찬가지로 **외부에서 파드에 접근할 수 있는 서비스 타입**
- 일반적으로 AWS, GCP 등과 같은 클라우드 플랫폼 환경에서 사용

**ExternalName**

- 외부 서비스를 쿠버네티스 내부에서 호출하고자 할 때 사용
- 클러스터 내의 파드에서 외부 IP 주소에 서비스의 이름으로 접근할 수 있음

### 서비스 생성 방법

1. **매니패스트(YAML)**
2. **kubectl expose**

## ClusterIP 타입의 서비스
![Alt text](/statics/ClusterIP.png)

## NodePort 타입의 서비스
- 모든 노드의 특정 포트를 개방해 서비스에 접근하는 방식  
- 노드의 IP 주소에 공개 포트(기본적으로 30000~32767 범위)를 오픈   
→ 클러스터 외부에서 클러스터 내부의 파드로 요청을 전달하는 것이 가능

![Alt text](/statics/NodePort.png)

## LoadBalancer 타입의 서비스
- 서버에 가해지는 부하를 분산해주는 장치 또는 기술을 통칭
- 일반적으로 온프레미스 환경에서는 NodePort 타입의 서비스를 사용하고, 퍼블릭 클라우드 환경에서는 LoadBalancer 타입의 서비스를 사용해서 애플리케이션을 외부에 노출

![Alt text](/statics/LoadBalancer.png)

**L4 로드밸런서**

- 네트워크 계층 또는 트랜스포트 계층의 정보를 기반으로 부하 분산
- IP 주소 또는 PORT 주소를 이용
- 쿠버네티스에서 말하는 일반적인 로드밸런서는 L4 로드밸런서의 기능을 수행

**L7 로드밸런서**

- 애플리케이션 계층의 정보를 기반으로 부하 분산
- URL, HTTP, Header, Cookie 등과 같은 사용자 요청 정보를 이용
- 쿠버네티스에서 L7 로드밸런서 기능을 인그레스(Ingress)를 사용해 구현

### **온프레미스 환경에서 LoadBalancer 타입의 서비스를 연동**

클러스터 내부에 로드밸런서 서비스를 받아주는 구성이 필요
  → MetalLB가 담당

  **MetalLB**
- 쿠버네티스 클러스터 내에서 로드 밸런싱을 제공하기 위한 오픈 소스 프로젝트
- 클라우드 공급자에서 실행되지 않는 클러스터에서 LoadBalancer 유형의 쿠버네티스 서비스 생성이 가능


## **ExternalName 타입의 서비스**

- 파드에서 쿠버네티스 클러스터 외부의 엔드포인트에 접속하기 위한 이름을 해결해 주는 역할  
    ex) 퍼블릭 클라우드의 데이터베이스 또는 인공지능 API 서비스 등에 접근할 때 사용

    ![Alt text](/statics/ExternalName.png)

# 인그레스(Ingress)

### 주요기능

- 외부요청 라우팅
- 가상호스트 기반의 요청처리
- SSL/TLS 등의 보안연결 처리

→ 인그레스를 사용하려면 **인그레스 컨트롤러**가 필요!